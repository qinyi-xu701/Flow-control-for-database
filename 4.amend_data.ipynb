{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deal with re-upload or patching data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hsudu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import pyodbc\n",
    "import shutil\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import date\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# home and time\n",
    "home = Path.home()\n",
    "todaystr = date.today().strftime('%Y-%m-%d')\n",
    "# targetFolder = os.path.join(home, 'HP Inc','GPSTW SOP - 2021 日新','Project team','Upload folder ( for buyer update )')\n",
    "if username == 'panj':\n",
    "    targetFolder = os.path.join(home, 'HP Inc','GPS TW Innovation - 文件','Project team','Project RiXin - Shortage management', 'Upload_folder') # Jesse\n",
    "else:\n",
    "    targetFolder = os.path.join(home, 'HP Inc','GPS TW Innovation - Documents','Project team','Project RiXin - Shortage management', 'Upload_folder') # Dustin\n",
    "\n",
    "# file path\n",
    "FD_path =  os.path.join(Path(home, targetFolder, 'FD_today','amend'))\n",
    "Shortage_path =  os.path.join(Path(home, targetFolder, 'Shortage_today','amend'))\n",
    "PNbasedDetail_path =  os.path.join(Path(home, targetFolder, 'PNbasedDetail_today','amend'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect SQL server\n",
    "conn = pyodbc.connect('Driver={SQL Server Native Client 11.0}; Server=g7w11206g.inc.hpicorp.net; Database=CSI; Trusted_Connection=Yes;')\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important maxlen function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxLen(df_all: pd.DataFrame, sort_index: list) -> pd.DataFrame:\n",
    "    # sort based on len\n",
    "    sort_list = []\n",
    "    for _ in sort_index:\n",
    "        try:\n",
    "            df_all[str('len_' + _)] = df_all[_].str.len()\n",
    "            sort_list.append(str('len_' + _))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    df_all = df_all.reset_index(drop = True)\n",
    "\n",
    "    max_files = []\n",
    "    for i, ele in enumerate(sort_list):\n",
    "        idmax = df_all[ele].max()\n",
    "        max = df_all[df_all[ele] == idmax]\n",
    "        max_files.append(max.head(1))\n",
    "    df_max_to_add = pd.concat(max_files).drop_duplicates()\n",
    "\n",
    "    df_max_to_add.index.values.sort()\n",
    "\n",
    "    # drop the max len row\n",
    "    for i, ele in enumerate(df_max_to_add.index.values):\n",
    "        df_all = df_all.drop([df_all.index[ele - i]])\n",
    "\n",
    "    # concat and put on the top\n",
    "    output = pd.concat([df_max_to_add, df_all]).reset_index( drop = True )\n",
    "\n",
    "    # cut more than 500\n",
    "    for _ in sort_index:\n",
    "        try:\n",
    "            output[_] = output[_].apply(lambda x: x[:450] if len(x) > 500 else x)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    # final step, drop calculate step and output\n",
    "    output = output.drop(columns = sort_list)\n",
    "    output['Item'] = output['Item'].astype(str)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete old FD and upload new FD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20240307', 'LCD', 'CEI', 'JenBella', 'FD.xlsx']\n",
      "4 rows deleted from CSI.OPS.GPS_tbl_ops_fd\n",
      "['20240307', 'LCD', 'CQIEC', 'JenBella', 'FD.xlsx']\n",
      "1 rows deleted from CSI.OPS.GPS_tbl_ops_fd\n",
      "['20240307', 'LCD', 'CQQCI', 'JenBella', 'FD.xlsx']\n",
      "7 rows deleted from CSI.OPS.GPS_tbl_ops_fd\n",
      "['20240307', 'SSS', 'CEI', 'ChenVera', 'FD.xlsx']\n",
      "2 rows deleted from CSI.OPS.GPS_tbl_ops_fd\n",
      "['20240307', 'SSS', 'WHFXN', 'ChenVera', 'FD.xlsx']\n",
      "4 rows deleted from CSI.OPS.GPS_tbl_ops_fd\n"
     ]
    }
   ],
   "source": [
    "# Get the names of all files in the folder with .xlsx extension\n",
    "file_names = [file_name for file_name in os.listdir(FD_path) if file_name.endswith('.xlsx')]\n",
    "\n",
    "for file in file_names:\n",
    "    condition = file.split('_')\n",
    "    print(condition)\n",
    "\n",
    "    # date condition\n",
    "    date_condition = datetime.datetime(int(condition[0][:4]),int(condition[0][4:6]),int(condition[0][6:]))\n",
    "    # commodity condition\n",
    "    commodity_condition = (condition[1])\n",
    "    # ODM condition\n",
    "    ODM_condition = (condition[2])\n",
    "    # buyer condition\n",
    "    buyer_condition = (condition[3])\n",
    "\n",
    "    # delete statement\n",
    "    delete_query = \"DELETE FROM CSI.OPS.GPS_tbl_ops_fd WHERE ReportDate = ? AND Commodity = ? AND ODM = ? AND BuyerName = ?\"\n",
    "    params = (date_condition, commodity_condition, ODM_condition, buyer_condition)\n",
    "    cursor.execute(delete_query, params)\n",
    "\n",
    "    # check result\n",
    "    print(f\"{cursor.rowcount} rows deleted from CSI.OPS.GPS_tbl_ops_fd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the fd\n",
    "fd_amend_temp = []\n",
    "\n",
    "# Loop through all the Excel files in the folder\n",
    "for fd in glob.glob(os.path.join(FD_path, '*.xlsx')):\n",
    "    # Read the data from the Excel file into a pandas dataframe\n",
    "    dff = pd.read_excel(fd)\n",
    "    # Append the dataframe to the list\n",
    "    fd_amend_temp.append(dff)\n",
    "\n",
    "# Concatenate all the dataframes into a single dataframe\n",
    "FD_amend_data_temp = pd.concat(fd_amend_temp, ignore_index=True)\n",
    "FD_amend_data_temp['FV'] = FD_amend_data_temp['FV'].str.strip()\n",
    "\n",
    "# Max len check\n",
    "try:\n",
    "    FD_amend_data = maxLen(FD_amend_data_temp, ['FV','Platform'])\n",
    "except:\n",
    "    FD_amend_data = FD_amend_data_temp.copy()\n",
    "\n",
    "for index, row in FD_amend_data.iterrows():\n",
    "    f_ODM = row['ODM']\n",
    "    f_Item = row['Item']\n",
    "    f_Commodity = row['Commodity']\n",
    "    f_FV = row['FV']\n",
    "    f_Platform = row['Platform']\n",
    "    f_HP_PN = row['HP_PN']\n",
    "    f_Supplier = row['Supplier']\n",
    "    f_PN = row['HP PN']\n",
    "    f_ReportDate = row['ReportDate']\n",
    "    f_FDdate = row['FDdate']\n",
    "    f_FDQty = row['FDQty']\n",
    "    f_BuyerName = row['BuyerName']\n",
    "    \n",
    "    cursor.execute(f\"INSERT INTO CSI.OPS.GPS_tbl_ops_fd ( ODM,Item,Commodity,FV,Platform,Supplier,[HP PN],FDdate,FDQty,Reportdate,BuyerName )\\\n",
    "                    VALUES('{f_ODM}','{f_Item}','{f_Commodity}','{f_FV}','{f_Platform}','{f_Supplier}','{f_PN}','{f_FDdate}','{f_FDQty}','{f_ReportDate}','{f_BuyerName}')\".replace(\"'NaT'\", \"NULL\"))\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete old shortage and upload new shortage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20240307', 'LCD', 'CEI', 'JenBella', 'Shortage.xlsx']\n",
      "1 rows deleted from CSI.OPS.GPS_tbl_ops_shortage_ext\n",
      "['20240307', 'LCD', 'CQIEC', 'JenBella', 'Shortage.xlsx']\n",
      "1 rows deleted from CSI.OPS.GPS_tbl_ops_shortage_ext\n",
      "['20240307', 'LCD', 'CQQCI', 'JenBella', 'Shortage.xlsx']\n",
      "2 rows deleted from CSI.OPS.GPS_tbl_ops_shortage_ext\n",
      "['20240307', 'SSS', 'CEI', 'ChenVera', 'Shortage.xlsx']\n",
      "1 rows deleted from CSI.OPS.GPS_tbl_ops_shortage_ext\n",
      "['20240307', 'SSS', 'WHFXN', 'ChenVera', 'Shortage.xlsx']\n",
      "2 rows deleted from CSI.OPS.GPS_tbl_ops_shortage_ext\n"
     ]
    }
   ],
   "source": [
    "# Get the names of all files in the folder with .xlsx extension\n",
    "file_names = [file_name for file_name in os.listdir(Shortage_path) if file_name.endswith('.xlsx')]\n",
    "\n",
    "for file in file_names:\n",
    "    condition = file.split('_')\n",
    "    print(condition)\n",
    "\n",
    "    # date condition\n",
    "    date_condition = datetime.datetime(int(condition[0][:4]),int(condition[0][4:6]),int(condition[0][6:]))\n",
    "    # commodity condition\n",
    "    commodity_condition = (condition[1])\n",
    "    # ODM condition\n",
    "    ODM_condition = (condition[2])\n",
    "    # buyer condition\n",
    "    buyer_condition = (condition[3])\n",
    "\n",
    "    # delete statement\n",
    "    delete_query = \"DELETE FROM CSI.OPS.GPS_tbl_ops_shortage_ext WHERE ReportDate = ? AND Commodity = ? AND ODM = ? AND BuyerName = ?\"\n",
    "    params = (date_condition, commodity_condition, ODM_condition, buyer_condition)\n",
    "    cursor.execute(delete_query, params)\n",
    "\n",
    "    # check result\n",
    "    print(f\"{cursor.rowcount} rows deleted from CSI.OPS.GPS_tbl_ops_shortage_ext\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the Shortage\n",
    "Shortage_amend_temp = []\n",
    "\n",
    "# Loop through all the Excel files in the folder\n",
    "for Shortage in glob.glob(os.path.join(Shortage_path, '*.xlsx')):\n",
    "    # Read the data from the Excel file into a pandas dataframe\n",
    "    dfs = pd.read_excel(Shortage)\n",
    "    # Append the dataframe to the list\n",
    "    Shortage_amend_temp.append(dfs)\n",
    "\n",
    "# Concatenate all the dataframes into a single dataframe\n",
    "Shortage_amend_data_temp = pd.concat(Shortage_amend_temp, ignore_index=True)\n",
    "Shortage_amend_data_temp['FV'] = Shortage_amend_data_temp['FV'].str.strip()\n",
    "\n",
    "\n",
    "# Max len check\n",
    "try:\n",
    "    Shortage_amend_data_temp['HP_PN'] = Shortage_amend_data_temp['HP_PN'].apply(lambda x: x[:128] if len(x) > 128 else x)\n",
    "except:    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    Shortage_amend_data = maxLen(Shortage_amend_data_temp, ['Platform','FV'])\n",
    "except:\n",
    "    Shortage_amend_data = Shortage_amend_data_temp.copy()\n",
    "\n",
    "for index, row in Shortage_amend_data.iterrows():\n",
    "    s_ODM = row['ODM']\n",
    "    s_Item = row['Item']\n",
    "    s_Commodity = row['Commodity']\n",
    "    s_FV = row['FV']\n",
    "    s_Platform = row['Platform']\n",
    "    s_P1 = row['P1']\n",
    "    s_P2 = row['Net P2']\n",
    "    s_P3 = row['Net P3']\n",
    "    s_Total = row['Total Shortage Qty']\n",
    "    s_BT = row['BT shortage']\n",
    "    s_working = row['Working on upside']\n",
    "    s_ReportDate = pd.to_datetime(row['ReportDate'])\n",
    "    s_lastFDdate = pd.to_datetime(row['last FD date'])\n",
    "    s_BuyerName = row['BuyerName']\n",
    "    s_PN = row['HP_PN']\n",
    "\n",
    "    cursor.execute(f\"INSERT INTO CSI.OPS.GPS_tbl_ops_shortage_ext ( ODM,Item,Commodity,FV,Platform,P1,[Net P2],[Net P3],[Total Shortage Qty],[BT shortage],[Working on upside],ReportDate,[last FD date],HP_PN,BuyerName )\\\n",
    "                    VALUES('{s_ODM}','{s_Item}','{s_Commodity}','{s_FV}','{s_Platform}','{s_P1}','{s_P2}','{s_P3}','{s_Total}','{s_BT}','{s_working}','{s_ReportDate}','{s_lastFDdate}','{s_PN}','{s_BuyerName}')\".replace(\"'NaT'\", \"NULL\"))\n",
    "\n",
    "conn.commit()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete old PNbasedDetail and upload new PNbasedDetail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20240307', 'LCD', 'CEI', 'JenBella', 'PNbasedDetail.xlsx']\n",
      "1 rows deleted from CSI.OPS.GPS_tbl_ops_PNbasedDetail\n",
      "['20240307', 'LCD', 'CQIEC', 'JenBella', 'PNbasedDetail.xlsx']\n",
      "1 rows deleted from CSI.OPS.GPS_tbl_ops_PNbasedDetail\n",
      "['20240307', 'LCD', 'CQQCI', 'JenBella', 'PNbasedDetail.xlsx']\n",
      "2 rows deleted from CSI.OPS.GPS_tbl_ops_PNbasedDetail\n",
      "['20240307', 'SSS', 'CEI', 'ChenVera', 'PNbasedDetail.xlsx']\n",
      "1 rows deleted from CSI.OPS.GPS_tbl_ops_PNbasedDetail\n",
      "['20240307', 'SSS', 'WHFXN', 'ChenVera', 'PNbasedDetail.xlsx']\n",
      "2 rows deleted from CSI.OPS.GPS_tbl_ops_PNbasedDetail\n"
     ]
    }
   ],
   "source": [
    "# Get the names of all files in the folder with .xlsx extension\n",
    "file_names = [file_name for file_name in os.listdir(PNbasedDetail_path) if file_name.endswith('.xlsx')]\n",
    "\n",
    "for file in file_names:\n",
    "    condition = file.split('_')\n",
    "    print(condition)\n",
    "\n",
    "    # date condition\n",
    "    date_condition = datetime.datetime(int(condition[0][:4]),int(condition[0][4:6]),int(condition[0][6:]))\n",
    "    # commodity condition\n",
    "    commodity_condition = (condition[1])\n",
    "    # ODM condition\n",
    "    ODM_condition = (condition[2])\n",
    "    # buyer condition\n",
    "    buyer_condition = (condition[3])\n",
    "\n",
    "    # delete statement\n",
    "    delete_query = \"DELETE FROM CSI.OPS.GPS_tbl_ops_PNbasedDetail WHERE ReportDate = ? AND Commodity = ? AND ODM = ? AND BuyerName = ?\"\n",
    "    params = (date_condition, commodity_condition, ODM_condition, buyer_condition)\n",
    "    cursor.execute(delete_query, params)\n",
    "\n",
    "    # check result\n",
    "    print(f\"{cursor.rowcount} rows deleted from CSI.OPS.GPS_tbl_ops_PNbasedDetail\" )\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the PNbasedDetail\n",
    "PNbasedDetail_amend_temp = []\n",
    "\n",
    "# Loop through all the Excel files in the folder\n",
    "for PNbasedDetail in glob.glob(os.path.join(PNbasedDetail_path, '*.xlsx')):\n",
    "    # Read the data from the Excel file into a pandas dataframe\n",
    "    dfp = pd.read_excel(PNbasedDetail)\n",
    "    # Append the dataframe to the list\n",
    "    PNbasedDetail_amend_temp.append(dfp)\n",
    "\n",
    "# Concatenate all the dataframes into a single dataframe\n",
    "PNbasedDetail_amend_data_temp = pd.concat(PNbasedDetail_amend_temp, ignore_index=True)\n",
    "\n",
    "# fillna for PNbasedDetail\n",
    "for i in ['GPS Remark', 'ODM use column1','ODM use column2','ODM use column3','ODM use column4','ODM use column5']:\n",
    "    PNbasedDetail_amend_data_temp[i] = PNbasedDetail_amend_data_temp[i].fillna(\"\")\n",
    "\n",
    "# Max len check\n",
    "try:\n",
    "    PNbasedDetail_amend_data = maxLen(PNbasedDetail_amend_data_temp, ['GPS Remark','ODM use column1','ODM use column2','ODM use column3','ODM use column4','ODM use column5'])\n",
    "except:\n",
    "    PNbasedDetail_amend_data = PNbasedDetail_amend_data_temp.copy()\n",
    "\n",
    "for index, row in PNbasedDetail_amend_data.iterrows():\n",
    "    p_ODM = row['ODM']\n",
    "    p_Item = row['Item']\n",
    "    p_Commodity = row['Commodity']\n",
    "    p_PN = row['HP PN']\n",
    "    p_Remark = str(row['GPS Remark']).replace(\"\\'\", \"\\'\\'\")\n",
    "    p_stock = row['852 stock']\n",
    "    p_change = row['852 stock change']\n",
    "    p_over = row['Over pull qty']\n",
    "    p_ODM1 = str(row['ODM use column1']).replace(\"\\'\", \"\\'\\'\")\n",
    "    p_ODM2 = str(row['ODM use column2']).replace(\"\\'\", \"\\'\\'\")\n",
    "    p_ODM3 = str(row['ODM use column3']).replace(\"\\'\", \"\\'\\'\")\n",
    "    p_ODM4 = str(row['ODM use column4']).replace(\"\\'\", \"\\'\\'\")\n",
    "    p_ODM5 = str(row['ODM use column5']).replace(\"\\'\", \"\\'\\'\")\n",
    "    p_ReportDate = row['ReportDate']\n",
    "    p_BuyerName = row['BuyerName']\n",
    "\n",
    "    cursor.execute(f\"INSERT INTO CSI.OPS.GPS_tbl_ops_PNbasedDetail ( ODM,Item,Commodity,[HP PN],[GPS Remark],[852 stock],[852 stock change],[Over pull qty],\\\n",
    "                    [ODM use column1],[ODM use column2],[ODM use column3],[ODM use column4],[ODM use column5],ReportDate,BuyerName )\\\n",
    "                    VALUES('{p_ODM}','{p_Item}','{p_Commodity}','{p_PN}','{p_Remark}','{p_stock}','{p_change}','{p_over}','{p_ODM1}','{p_ODM2}','{p_ODM3}','{p_ODM4}','{p_ODM5}','{p_ReportDate}','{p_BuyerName}')\".replace(\"'NaT'\", \"NULL\"))\n",
    "\n",
    "conn.commit()\n",
    "# conn.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Move uploaded data to archieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FD Archieve folder define\n",
    "FD_archive_folder = os.path.join(targetFolder, 'FD_Archive')\n",
    "# Move FD\n",
    "for f in os.listdir(FD_path):\n",
    "    if f.endswith('.xlsx'):\n",
    "        shutil.move(os.path.join(FD_path, f), os.path.join(FD_archive_folder, f))\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "# Shortage Archieve folder define\n",
    "shortage_archive_folder = os.path.join(targetFolder ,\"Shortage_Archive\")\n",
    "# Move Shortage\n",
    "for f in os.listdir(Shortage_path):\n",
    "    if f.endswith('.xlsx'):\n",
    "        shutil.move(os.path.join(Shortage_path, f), os.path.join(shortage_archive_folder, f))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# PNbasedDetail Archieve folder define\n",
    "PNbasedDetail_archive_folder = os.path.join(targetFolder ,\"PNbasedDetail_Archive\")\n",
    "# Move PNbasedDetail\n",
    "for f in os.listdir(PNbasedDetail_path):\n",
    "    if f.endswith('.xlsx'):\n",
    "        shutil.move(os.path.join(PNbasedDetail_path, f), os.path.join(PNbasedDetail_archive_folder, f))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "PNbasedDetail_archive_folder = os.path.join(targetFolder ,\"PNbasedDetail_Archive\")\n",
    "# Move PNbasedDetail\n",
    "for f in os.listdir(PNbasedDetail_path):\n",
    "    if f.endswith('.xlsx'):\n",
    "        shutil.move(os.path.join(PNbasedDetail_path, f), os.path.join(PNbasedDetail_archive_folder, f))\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add process to remove duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 rows deleted from CSI.OPS.GPS_tbl_ops_shortage_ext\n",
      "2 rows deleted from CSI.OPS.GPS_tbl_ops_shortage_ext\n",
      "2 rows deleted from CSI.OPS.GPS_tbl_ops_shortage_ext\n",
      "2 rows deleted from CSI.OPS.GPS_tbl_ops_shortage_ext\n",
      "1 rows uploaded to CSI.OPS.GPS_tbl_ops_shortage_ext\n",
      "1 rows uploaded to CSI.OPS.GPS_tbl_ops_shortage_ext\n",
      "1 rows uploaded to CSI.OPS.GPS_tbl_ops_shortage_ext\n",
      "1 rows uploaded to CSI.OPS.GPS_tbl_ops_shortage_ext\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\" SELECT * FROM CSI.OPS.GPS_tbl_ops_shortage_ext\")\n",
    "# find the rows that are not on database\n",
    "allshortage = pd.DataFrame.from_records(cursor.fetchall(), columns = [i[0] for i in cursor.description])\n",
    "\n",
    "# extract duplicated rows\n",
    "duplicate_rows = allshortage[allshortage.duplicated()]\n",
    "duplicate_rows = duplicate_rows[duplicate_rows['ReportDate'] > '2023-01-01']\n",
    "# delete them all in database\n",
    "for index,row in duplicate_rows.iterrows():\n",
    "    delete_query = \"DELETE FROM CSI.OPS.GPS_tbl_ops_shortage_ext WHERE ReportDate =? AND ODM=? AND Item=? AND Commodity=? AND FV=? AND Platform=? AND BuyerName=? \"\n",
    "    cursor.execute(delete_query,(row['ReportDate'],row['ODM'],row['Item'],row['Commodity'],row['FV'],row['Platform'],row['BuyerName']))\n",
    "    if cursor.rowcount:\n",
    "        # print(row)\n",
    "        print(f\"{cursor.rowcount} rows deleted from CSI.OPS.GPS_tbl_ops_shortage_ext\")\n",
    "conn.commit()\n",
    "# upload them in database\n",
    "for index, row in duplicate_rows.iterrows():\n",
    "    s_ODM = row['ODM']\n",
    "    s_Item = row['Item']\n",
    "    s_Commodity = row['Commodity']\n",
    "    s_FV = row['FV']\n",
    "    s_Platform = row['Platform']\n",
    "    s_P1 = row['P1']\n",
    "    s_P2 = row['Net P2']\n",
    "    s_P3 = row['Net P3']\n",
    "    s_Total = row['Total Shortage Qty']\n",
    "    s_BT = row['BT shortage']\n",
    "    s_working = row['Working on upside']\n",
    "    s_ReportDate = pd.to_datetime(row['ReportDate'])\n",
    "    s_lastFDdate = pd.to_datetime(row['last FD date'])\n",
    "    s_BuyerName = row['BuyerName']\n",
    "    s_PN = row['HP_PN']\n",
    "\n",
    "    cursor.execute(f\"INSERT INTO CSI.OPS.GPS_tbl_ops_shortage_ext ( ODM,Item,Commodity,FV,Platform,P1,[Net P2],[Net P3],[Total Shortage Qty],[BT shortage],[Working on upside],ReportDate,[last FD date],HP_PN,BuyerName )\\\n",
    "                    VALUES('{s_ODM}','{s_Item}','{s_Commodity}','{s_FV}','{s_Platform}','{s_P1}','{s_P2}','{s_P3}','{s_Total}','{s_BT}','{s_working}','{s_ReportDate}','{s_lastFDdate}','{s_PN}','{s_BuyerName}')\".replace(\"'NaT'\", \"NULL\"))\n",
    "    if cursor.rowcount:\n",
    "        # print(row)\n",
    "        print(f\"{cursor.rowcount} rows uploaded to CSI.OPS.GPS_tbl_ops_shortage_ext\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cadcbe1cf7f77607d04fb86883766795fe82998168b094e073d163801885097d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
